@misc{baek2023knowledgeaugmentedlanguagemodelprompting,
      title={Knowledge-Augmented Language Model Prompting for Zero-Shot Knowledge Graph Question Answering}, 
      author={Jinheon Baek and Alham Fikri Aji and Amir Saffari},
      year={2023},
      eprint={2306.04136},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2306.04136}, 
}

@inproceedings{wu2019zero,
 title={Zero-shot Entity Linking with Dense Entity Retrieval},
 author={Ledell Wu and Fabio Petroni and Martin Josifoski and Sebastian Riedel and Luke Zettlemoyer},
 booktitle={EMNLP},
 year={2020}
}

@inproceedings{oguz-etal-2022-unik,
    title = "{U}ni{K}-{QA}: Unified Representations of Structured and Unstructured Knowledge for Open-Domain Question Answering",
    author = "Oguz, Barlas  and
      Chen, Xilun  and
      Karpukhin, Vladimir  and
      Peshterliev, Stan  and
      Okhonko, Dmytro  and
      Schlichtkrull, Michael  and
      Gupta, Sonal  and
      Mehdad, Yashar  and
      Yih, Scott",
    editor = "Carpuat, Marine  and
      de Marneffe, Marie-Catherine  and
      Meza Ruiz, Ivan Vladimir",
    booktitle = "Findings of the Association for Computational Linguistics: NAACL 2022",
    month = jul,
    year = "2022",
    address = "Seattle, United States",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.findings-naacl.115",
    doi = "10.18653/v1/2022.findings-naacl.115",
    pages = "1535--1546",
    abstract = "We study open-domain question answering with structured, unstructured and semi-structured knowledge sources, including text, tables, lists and knowledge bases. Departing from prior work, we propose a unifying approach that homogenizes all sources by reducing them to text and applies the retriever-reader model which has so far been limited to text sources only. Our approach greatly improves the results on knowledge-base QA tasks by 11 points, compared to latest graph-based methods. More importantly, we demonstrate that our unified knowledge (UniK-QA) model is a simple and yet effective way to combine heterogeneous sources of knowledge, advancing the state-of-the-art results on two popular question answering benchmarks, NaturalQuestions and WebQuestions, by 3.5 and 2.6 points, respectively. The code of UniK-QA is available at: \url{https://github.com/facebookresearch/UniK-QA}.",
}

@inproceedings{ren-etal-2021-rocketqav2,
    title = "{R}ocket{QA}v2: A Joint Training Method for Dense Passage Retrieval and Passage Re-ranking",
    author = "Ren, Ruiyang  and
      Qu, Yingqi  and
      Liu, Jing  and
      Zhao, Wayne Xin  and
      She, QiaoQiao  and
      Wu, Hua  and
      Wang, Haifeng  and
      Wen, Ji-Rong",
    editor = "Moens, Marie-Francine  and
      Huang, Xuanjing  and
      Specia, Lucia  and
      Yih, Scott Wen-tau",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.224",
    doi = "10.18653/v1/2021.emnlp-main.224",
    pages = "2825--2835",
    abstract = "In various natural language processing tasks, passage retrieval and passage re-ranking are two key procedures in finding and ranking relevant information. Since both the two procedures contribute to the final performance, it is important to jointly optimize them in order to achieve mutual improvement. In this paper, we propose a novel joint training approach for dense passage retrieval and passage reranking. A major contribution is that we introduce the dynamic listwise distillation, where we design a unified listwise training approach for both the retriever and the re-ranker. During the dynamic distillation, the retriever and the re-ranker can be adaptively improved according to each other{'}s relevance information. We also propose a hybrid data augmentation strategy to construct diverse training instances for listwise training approach. Extensive experiments show the effectiveness of our approach on both MSMARCO and Natural Questions datasets. Our code is available at \url{https://github.com/PaddlePaddle/RocketQA}.",
}

@article{INR-019,
url = {http://dx.doi.org/10.1561/1500000019},
year = {2009},
volume = {3},
journal = {Foundations and TrendsÂ® in Information Retrieval},
title = {The Probabilistic Relevance Framework: BM25 and Beyond},
doi = {10.1561/1500000019},
issn = {1554-0669},
number = {4},
pages = {333-389},
author = {Stephen Robertson and Hugo Zaragoza}
}

@article{47761,title	= {Natural Questions: a Benchmark for Question Answering Research},author	= {Tom Kwiatkowski and Jennimaria Palomaki and Olivia Redfield and Michael Collins and Ankur Parikh and Chris Alberti and Danielle Epstein and Illia Polosukhin and Matthew Kelcey and Jacob Devlin and Kenton Lee and Kristina N. Toutanova and Llion Jones and Ming-Wei Chang and Andrew Dai and Jakob Uszkoreit and Quoc Le and Slav Petrov},year	= {2019},journal	= {Transactions of the Association of Computational Linguistics}}

@misc{baek2023knowledgeaugmentedlanguagemodelverification,
      title={Knowledge-Augmented Language Model Verification}, 
      author={Jinheon Baek and Soyeong Jeong and Minki Kang and Jong C. Park and Sung Ju Hwang},
      year={2023},
      eprint={2310.12836},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2310.12836}, 
}



