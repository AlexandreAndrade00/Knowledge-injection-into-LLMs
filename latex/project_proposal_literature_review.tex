\typeout{IJCAI--24 Instructions for Authors}

% These are the instructions for authors for IJCAI-24.

\documentclass{article}
\pdfpagewidth=8.5in
\pdfpageheight=11in

% The file ijcai24.sty is a copy from ijcai22.sty
% The file ijcai22.sty is NOT the same as previous years'
\usepackage{ijcai24}

% Use the postscript times font!
\usepackage{times}
\usepackage{soul}
\usepackage{url}
\usepackage[hidelinks]{hyperref}
\usepackage[utf8]{inputenc}
\usepackage[small]{caption}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage[switch]{lineno}

\usepackage[
backend=biber,
style=alphabetic
]{biblatex}


\addbibresource{bibliography.bib}

% Comment out this line in the camera-ready submission
\linenumbers

\urlstyle{same}


% PDF Info Is REQUIRED.

% Please leave this \pdfinfo block untouched both for the submission and
% Camera Ready Copy. Do not include Title and Author information in the pdfinfo section
\pdfinfo{
/TemplateVersion (IJCAI.2024.0)
}

\title{Project Proposal and Literature Review}


% Single author syntax
\author{
    Alexandre Domingues Andrade
    \emails
    alexandrade@student.dei.uc.pt
}

\begin{document}

\maketitle

\section{Project Proposal}

\subsection{Description}

The use of Large Language Models (LLMs) by end users have significantly increased since the announcement of the ChatGPT by OpenAI, this use increase can lead to spread of miss information if the model hallucinate and the user don't search in multiple sources.

The objective of this project is to reduce the hallucination of the LLMs with knowledge injection from Knowledge Graphs such as DBPedia and benchmark the performance of the enriched LLMs.

\subsection{Goals}

To improve the factual quality of the generated text it is necessary to inject the knowledge in the inference pipeline, in this project two methods will be used. Both methods don't required to train the pre-trained model or fine-tuning it.

The first one is user input enrichment, before feeding the input to the model it is introduced data from the knowledge graphs using keywords present in the input. This is a zero-shot method, doesn't require any type of training.

The second method involves injecting the data directly in the model, different approaches exits to accomplish this but the objective is to use a pre-trained model and use a adapter based architecture.

In order to evaluate the quality of the solutions, first they will be compared with the model without the architectural modifications and in a question answering benchmark.

\subsection{Checkpoints}

\begin{enumerate}
	\item October 29
	\item November 19
\end{enumerate}

\subsection{Data and Tools}

Dataset - https://ai.google.com/research/NaturalQuestions/dataset

\section{Literature Review}

The input enrichment has inspiration in the work done on \cite{baek2023knowledgeaugmentedlanguagemodelprompting}, the objective of this project is to do incremental improvements.

Before feeding the user question to the LLM this approach give a prompt to the LLM with contextual information extracted from Knowledge Graphs. In order to accomplish this, firstly the entities are extracted from the introduced question with BLINK \cite{wu2019zero}, and the triplets found in the knowledge graph that correspond to the entities are extracted and saved with the question.

Then, it's necessary to verbalize the extracted triplets, in the paper it was used linear verbalization, but as mentioned, more advanced methods can be used to this task, it will be adopted a heuristics method \cite{oguz-etal-2022-unik}. To rank the verbalized information, it is necessary to measure the relevance of the sentences, BM25 \cite{INR-019} appeared with ground breaking results and since then multiple works have been done in this area, the RocketQA \cite{ren-etal-2021-rocketqav2} dense retriever based on pre-trained language model (PLM) will be used as result of the state-of-art results achieved when compared to another methods.

The top sentences are prompted to the model and finally the answer it's introduced in the model. 

\printbibliography

\end{document}
