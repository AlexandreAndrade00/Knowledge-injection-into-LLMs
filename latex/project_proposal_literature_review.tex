\typeout{IJCAI--24 Instructions for Authors}

% These are the instructions for authors for IJCAI-24.

\documentclass{article}
\pdfpagewidth=8.5in
\pdfpageheight=11in

% The file ijcai24.sty is a copy from ijcai22.sty
% The file ijcai22.sty is NOT the same as previous years'
\usepackage{ijcai24}

% Use the postscript times font!
\usepackage{times}
\usepackage{soul}
\usepackage{url}
\usepackage[hidelinks]{hyperref}
\usepackage[utf8]{inputenc}
\usepackage[small]{caption}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage[switch]{lineno}

\usepackage[
backend=biber,
style=alphabetic
]{biblatex}


\addbibresource{bibliography.bib}

% Comment out this line in the camera-ready submission
\linenumbers

\urlstyle{same}


% PDF Info Is REQUIRED.

% Please leave this \pdfinfo block untouched both for the submission and
% Camera Ready Copy. Do not include Title and Author information in the pdfinfo section
\pdfinfo{
/TemplateVersion (IJCAI.2024.0)
}

\title{Project Proposal and Literature Review}


% Single author syntax
\author{
    Alexandre Domingues Andrade
    \emails
    alexandrade@student.dei.uc.pt
}

\begin{document}

\maketitle

\section{Literature Review}

\section{Project Proposal}

\subsection{Description}

The use of Large Language Models (LLMs) by end users have significantly increased since the announcement of the ChatGPT by OpenAI. The amount of trust put in the models by the users can lead to spread of miss information because the users are miss-leaded by the models hallucinations.

The objective of this project is to improve the factual data given by the LLMs in its responses, reducing the hallucination and providing sources through knowledge injection from Knowledge Graphs such as Wikidata.

\subsection{Goals}

To improve the factual quality of the generated text, it is necessary to inject the knowledge somewhere in the neural network, in this project two methods will be used. Performance and efficiency wise, it is pretended to spend as little resource as possible, consequently, only pre-trained language models (PLM) will be used giving priority to zero-shot, one-shot, few-shot and adapter based methods.

\subsubsection{First approach}

The input prompt engineering has inspiration in the work done on \cite{baek2023knowledgeaugmentedlanguagemodelprompting}, the objective of this project is to increment what have been done.

Before feeding the user question to the PLM, this approach give a prompt to the PLM with contextual information extracted from Knowledge Graphs. It is necessary to identify the entities present in the sentence, to accomplish this, the entities are extracted from the question using BLINK \cite{wu2019zero}. BLINK is a entity linking algorithm that uses the Wikipedia as the knowledge base (entity library). The returned entities have an id and a description, this identification will be used to extract triplets from knowledge graphs and saved them with the question. 

Then, it's necessary to verbalize the extracted triplets, in the paper it was used linear verbalization, but as mentioned, more advanced methods can be used to this task, it will be adopted a heuristics method \cite{oguz-etal-2022-unik}. The triplets extraction lead to an exaggerated amount of context, most of them being irrelevant to the question, so it's necessary a ranking of the information. To rank the verbalized information, it is necessary to measure the relevance of the sentences, BM25 \cite{INR-019} appeared with ground breaking results and since then, multiple works have been done in this area, the RocketQA \cite{ren-etal-2021-rocketqav2} dense retriever based on pre-trained language model (PLM) will be used because of its achieved state-of-art results, when compared to other methods.

At last, the \textit{k} top sentences are prompted to the model and the question is fed.


\subsubsection{Second approach}

The second method involves injecting the data directly in the model, using an adapter based architecture. This approach only if the first don't take all available time, so further details will be investigated later.


\subsubsection{Benchmarking}

In order to evaluate the quality of the solutions, the will be benchmarked in Natural Questions Benchmark \cite{47761}.


\subsection{Checkpoints}

\begin{enumerate}
	\item October 29
	\item November 19
\end{enumerate}

\subsection{Data and Tools}

It will be used the Natural Questions dataset. The work will be done with python, with the PyTorch framework.


\printbibliography

\end{document}
